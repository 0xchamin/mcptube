# ğŸ¬ mcptube

**Convert any YouTube video into an AI-queryable MCP server.**

YouTube URL in â†’ searchable library â†’ ask your AI anything about any video.

mcptube extracts transcripts, metadata, and frames from YouTube videos, indexes them into a local vector database, and exposes everything as [MCP](https://modelcontextprotocol.io/) tools â€” queryable by Claude, ChatGPT, VS Code Copilot, Cursor, Gemini, and any MCP-compatible client.

---

## âœ¨ Features

- **Semantic search** across video transcripts (single video or entire library)
- **Frame extraction** at any timestamp or by natural language query
- **Auto-classification** with LLM-generated tags
- **Illustrated reports** â€” single-video or cross-video, markdown or HTML
- **Video discovery** â€” search YouTube by topic, filter and cluster results
- **Cross-video synthesis** â€” themes, agreements, and contradictions across videos
- **Dual interface** â€” full CLI + MCP server
- **Passthrough LLM** â€” MCP tools require zero API keys; the client LLM does the reasoning
- **BYOK** â€” CLI mode supports 100+ LLM providers via LiteLLM
- **Smart video resolver** â€” reference videos by ID, index, or title substring

---

## ğŸ“‹ Prerequisites

- **Python 3.12+**
- **ffmpeg** â€” required for frame extraction
  ```bash
  # macOS
  brew install ffmpeg

  # Ubuntu/Debian
  sudo apt install ffmpeg

  # Windows
  winget install ffmpeg
  ```

---

## ğŸš€ Installation

```bash
pip install mcptube
```

### From source (development)

```bash
git clone https://github.com/0xchamin/mcptube.git
cd mcptube
python -m venv .venv
source .venv/bin/activate
pip install -e ".[dev]"
```

---

## âš¡ Quick Start

### CLI

```bash
# Add a video
mcptube add "https://www.youtube.com/watch?v=dQw4w9WgXcQ"

# List your library
mcptube list

# Search across all videos
mcptube search "machine learning basics"

# Extract a frame
mcptube frame 1 120.5

# Start the MCP server
mcptube serve
```

### MCP Server

```bash
# Streamable HTTP (default) â€” works with Claude Code, ChatGPT
mcptube serve

# stdio â€” works with VS Code, Claude Desktop, Cursor
mcptube serve --stdio
```

---

## ğŸ”§ CLI Commands

| Command | Description |
|---------|-------------|
| `mcptube add <url>` | Ingest a YouTube video |
| `mcptube list` | List all videos in the library |
| `mcptube info <query>` | Show video details (ID, index, or text) |
| `mcptube remove <query>` | Remove a video from the library |
| `mcptube search <query>` | Semantic search across transcripts |
| `mcptube frame <video> <timestamp>` | Extract a frame at a timestamp |
| `mcptube frame-query <video> <text>` | Search transcript + extract frame |
| `mcptube classify <video>` | Auto-classify with LLM tags (BYOK) |
| `mcptube report <video>` | Generate an illustrated report (BYOK) |
| `mcptube report-query <query>` | Cross-video report from search (BYOK) |
| `mcptube discover <topic>` | Search YouTube + cluster results (BYOK) |
| `mcptube synthesize-cmd <topic> -v <id>` | Cross-video synthesis (BYOK) |
| `mcptube serve` | Start MCP server (Streamable HTTP) |
| `mcptube serve --stdio` | Start MCP server (stdio) |

### Smart Video Resolver

Commands that take a `<video>` or `<query>` argument accept:

| Input | Resolution |
|-------|-----------|
| `BpibZSMGtdY` | Exact YouTube video ID |
| `1` | Index number from `mcptube list` |
| `"prompting"` | Substring match on title or channel |

### Search Options

```bash
# Search all videos
mcptube search "attention mechanism"

# Search within a specific video
mcptube search "attention" --video "prompting"

# Limit results
mcptube search "attention" --limit 5
```

### Report Options

```bash
# Full report for a video
mcptube report "prompting" --format html --output report.html

# Focused report
mcptube report "prompting" --focus "reasoning strategies" --output focused.html

# Cross-video report from search
mcptube report-query "prompt engineering" --format html --output multi.html

# Cross-video synthesis
mcptube synthesize-cmd "prompting" -v BpibZSMGtdY -v UPGB-hsAoVY --output synthesis.html
```

---

## ğŸ¤– MCP Tools (13 tools)

All MCP tools use the **passthrough pattern** â€” no API key required on the server. The connected AI client (Claude, ChatGPT, Copilot) provides the LLM reasoning.

| Tool | Description |
|------|-------------|
| `add_video(url)` | Ingest a YouTube video |
| `remove_video(video_id)` | Remove from library |
| `list_videos()` | List all videos with metadata |
| `get_info(video_id)` | Full video details with transcript |
| `search(query, video_id?, limit)` | Semantic search (single or all videos) |
| `search_library(query, tags?, limit)` | Cross-library search with tag filter |
| `get_frame(video_id, timestamp)` | Extract frame at timestamp |
| `get_frame_by_query(video_id, query)` | Search + extract frame |
| `classify_video(video_id)` | Return metadata for client classification |
| `generate_report(video_id, query?)` | Return data for client report generation |
| `generate_report_from_query(query, tags?)` | Cross-video report data |
| `discover_videos(topic)` | YouTube search results |
| `synthesize(video_ids, topic)` | Cross-video synthesis data |

---

## ğŸ”Œ MCP Client Configuration

### Claude Code

```bash
# Streamable HTTP (recommended)
claude mcp add --transport http --scope global mcptube http://127.0.0.1:9093/mcp
```

> **Note:** Use `--scope global` to make mcptube available in all projects. Without it, the server is scoped to the directory where you ran the command.

Then start the server in a separate terminal:

```bash
mcptube serve
```

### VS Code / Copilot Chat

Add to `.vscode/mcp.json` in your workspace:

```json
{
  "servers": {
    "mcptube": {
      "command": "mcptube",
      "args": ["serve", "--stdio"]
    }
  }
}
```

> **Note:** If VS Code can't find `mcptube`, use the full path to the executable:
> ```json
> "command": "/path/to/your/.venv/bin/mcptube"
> ```
> Or if installed globally via pip, the command should work as-is.

### Claude Desktop

Add to `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "mcptube": {
      "command": "mcptube",
      "args": ["serve", "--stdio"]
    }
  }
}
```

### Cursor

Add to `.cursor/mcp.json`:

```json
{
  "mcpServers": {
    "mcptube": {
      "command": "mcptube",
      "args": ["serve", "--stdio"]
    }
  }
}
```

### ChatGPT

```
Settings â†’ Connectors â†’ Add â†’ http://localhost:9093/mcp
```

### Gemini CLI

Add to `settings.json`:

```json
{
  "mcpServers": {
    "mcptube": {
      "command": "mcptube",
      "args": ["serve", "--stdio"]
    }
  }
}
```

---

## ğŸ”‘ BYOK â€” Bring Your Own Key (CLI Mode)

CLI commands that use LLM features (classify, report, discover, synthesize) require an API key via environment variables:

```bash
# Anthropic (Claude)
export ANTHROPIC_API_KEY="sk-ant-..."

# OpenAI
export OPENAI_API_KEY="sk-..."

# Google (Gemini)
export GOOGLE_API_KEY="AI..."
```

mcptube auto-detects which key is available. Set a default model:

```bash
export MCPTUBE_DEFAULT_MODEL="anthropic/claude-sonnet-4-20250514"
```

> **Security:** Never pass API keys as CLI flags. Always use environment variables.

> **MCP mode does not need any API key** â€” the connected AI client provides the LLM.

---

## âš™ï¸ Configuration

All settings can be overridden via `MCPTUBE_`-prefixed environment variables:

| Variable | Default | Description |
|----------|---------|-------------|
| `MCPTUBE_DATA_DIR` | `~/.mcptube` | Root directory for all data |
| `MCPTUBE_HOST` | `127.0.0.1` | Server bind host |
| `MCPTUBE_PORT` | `9093` | Server bind port |
| `MCPTUBE_DEFAULT_MODEL` | `gpt-4o` | Default LLM model for CLI |

### Server Options

```bash
mcptube serve                              # Streamable HTTP on 127.0.0.1:9093
mcptube serve --stdio                      # stdio transport
mcptube serve --host 0.0.0.0 --port 8080   # Custom host/port
mcptube serve --reload                     # Dev mode with hot-reload
```

---

## ğŸ—ï¸ Architecture

```
CLI (Typer)  â†â”€â”€â”€â”€â”€â”€â”
                     â”œâ”€â”€ Service Layer (McpTubeService)
MCP Server (FastMCP) â†â”€â”˜        â”‚
                           â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                     Repository    VectorStore
                     (SQLite)      (ChromaDB)
                           â”‚
                     Ingestion Layer
                     â”œâ”€â”€ YouTubeExtractor (yt-dlp)
                     â”œâ”€â”€ FrameExtractor (yt-dlp + ffmpeg)
                     â”œâ”€â”€ LLMClient (LiteLLM â€” CLI only)
                     â”œâ”€â”€ ReportBuilder (CLI only)
                     â””â”€â”€ VideoDiscovery (CLI only)
```

### LLM Strategy

| Mode | LLM | Cost |
|------|-----|------|
| **CLI** | LiteLLM (BYOK) | User's API key |
| **MCP** | Client LLM (passthrough) | Free â€” client provides reasoning |

### Storage

| Component | Per video (~40 min) | 100 videos |
|-----------|-------------------|------------|
| SQLite (metadata + transcript) | ~200-500 KB | ~50 MB |
| ChromaDB (384-dim vectors) | ~1.5-2 MB | ~200 MB |
| **Total** | | **~250 MB** |

ChromaDB downloads the `all-MiniLM-L6-v2` embedding model (~80 MB) on first use. This is a one-time download cached at `~/.cache/chroma/`.

---

## ğŸ› ï¸ Tech Stack

- **FastMCP 3.0** â€” MCP server framework (Streamable HTTP + stdio)
- **yt-dlp** â€” YouTube extraction (transcripts, metadata, search)
- **ffmpeg** â€” On-demand frame extraction
- **ChromaDB** â€” Local vector database with built-in embeddings
- **LiteLLM** â€” Unified LLM interface (100+ providers)
- **Typer** â€” CLI framework
- **Pydantic** â€” Data models and settings
- **SQLite** â€” Library metadata storage

---

## ğŸ§ª Development

```bash
# Install dev dependencies
pip install -e ".[dev]"

# Run tests
pytest

# Run with coverage
pytest --cov=mcptube --cov-report=html

# Lint
ruff check src/

# Format
ruff format src/
```

---

## ğŸ“¦ Project Structure

```
mcptube/
â”œâ”€â”€ src/mcptube/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py              # Typer CLI
â”‚   â”œâ”€â”€ server.py           # FastMCP MCP server
â”‚   â”œâ”€â”€ service.py          # Core business logic
â”‚   â”œâ”€â”€ models.py           # Pydantic domain models
â”‚   â”œâ”€â”€ config.py           # Settings (pydantic-settings)
â”‚   â”œâ”€â”€ llm.py              # LiteLLM wrapper (BYOK)
â”‚   â”œâ”€â”€ report.py           # ReportBuilder
â”‚   â”œâ”€â”€ discovery.py        # VideoDiscovery
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ youtube.py      # YouTubeExtractor
â”‚   â”‚   â””â”€â”€ frames.py       # FrameExtractor
â”‚   â””â”€â”€ storage/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ repository.py   # Abstract VideoRepository
â”‚       â”œâ”€â”€ sqlite.py       # SQLiteVideoRepository
â”‚       â””â”€â”€ vectorstore.py  # VectorStore + ChromaVectorStore
â”œâ”€â”€ tests/
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸ—ºï¸ Roadmap

- [x] MVP â€” 13 MCP tools, CLI, semantic search, frames, reports
- [ ] MCP Apps â€” Interactive HTML UIs inline in chat
- [ ] Playlist / channel import
- [ ] Speaker diarization
- [ ] OCR on frames
- [ ] Auto-chaptering
- [ ] Multi-language transcripts
- [ ] SaaS tier (OAuth, pgvector, team libraries)

---

## ğŸ“„ License

MIT

---

Built with [FastMCP](https://gofastmcp.com) âš¡
